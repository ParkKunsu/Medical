{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f280574",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b723a",
   "metadata": {},
   "source": [
    "# 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d45102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:21:43.746423: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 19:21:43.755435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745749303.765091  163023 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745749303.768107  163023 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745749303.776000  163023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745749303.776008  163023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745749303.776009  163023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745749303.776010  163023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 19:21:43.778751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 인덱스:\n",
      " {'커피': 1, '한잔': 2, '어때': 3}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "\n",
    "# 전처리할 텍스트\n",
    "text = \"커피 한잔 어때\"\n",
    "\n",
    "# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# texts_to_sequences로 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# 단어 인덱스 확인\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\n단어 인덱스:\\n\", word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb97a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 카운트:\n",
      " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
      "\n",
      "문장 카운트:  3\n",
      "\n",
      "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
      " defaultdict(<class 'int'>, {'토큰화합니다': 1, '각': 1, '먼저': 1, '텍스트의': 2, '나누어': 1, '단어를': 1, '딥러닝에서': 2, '단어로': 1, '인식됩니다': 1, '토큰화해야': 1, '사용할': 1, '있습니다': 1, '수': 1, '토큰화한': 1, '결과는': 1})\n",
      "\n",
      "각 단어에 매겨진 인덱스 값:\n",
      " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도수 세기\n",
    "\n",
    "# 전처리하려는 세 개의 문장\n",
    "docs = [\n",
    "    \"먼저 텍스트의 각 단어를 나누어 토큰화합니다.\",\n",
    "    \"텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.\",\n",
    "    \"토큰화한 결과는 딥러닝에서 사용할 수 있습니다.\",\n",
    "]\n",
    "\n",
    "# 토큰화 함수를 이용해 전처리 하는 과정\n",
    "tokenizer = Tokenizer()  # 토큰화 함수 지정\n",
    "tokenizer.fit_on_texts(docs)  # 토큰화 함수에 문장 적용\n",
    "\n",
    "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n",
    "print(\"\\n단어 카운트:\\n\", tokenizer.word_counts)\n",
    "\n",
    "# 출력되는 순서는 랜덤\n",
    "print(\"\\n문장 카운트: \", tokenizer.document_count)\n",
    "print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", tokenizer.word_docs)\n",
    "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b29e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "단어 인덱스:\n",
      " {'커피': 1, '한잔': 2, '어때': 3, '오늘': 4, '날씨': 5, '참': 6, '좋네': 7, '옷이': 8, '어울려요': 9}\n",
      "\n",
      "시퀀스:\n",
      " [[1, 2, 3], [4, 5, 6, 7], [8, 9]]\n",
      "\n",
      "패딩된 시퀀스:\n",
      " [[0 1 2 3]\n",
      " [4 5 6 7]\n",
      " [0 0 8 9]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "임베딩 결과:\n",
      " [[[-0.00516986 -0.0460203  -0.02838464 -0.0175368  -0.01648979]\n",
      "  [ 0.02151359  0.03824228 -0.04767111 -0.04552304  0.00142624]\n",
      "  [ 0.01315511 -0.04433054 -0.00670988 -0.00583363  0.04794557]\n",
      "  [ 0.02239678  0.04616595  0.04050764  0.0230614   0.0278798 ]]\n",
      "\n",
      " [[-0.0196972   0.0141312   0.02223784  0.0423472  -0.04783505]\n",
      "  [ 0.02769143 -0.02929622  0.03053793  0.0310886   0.01711081]\n",
      "  [ 0.00831041 -0.00232695 -0.01993775  0.02003374 -0.04243187]\n",
      "  [-0.02441391  0.04234351 -0.03647687 -0.0179122  -0.04893551]]\n",
      "\n",
      " [[-0.00516986 -0.0460203  -0.02838464 -0.0175368  -0.01648979]\n",
      "  [-0.00516986 -0.0460203  -0.02838464 -0.0175368  -0.01648979]\n",
      "  [-0.03595823  0.03590194 -0.01707522  0.02883646 -0.01041295]\n",
      "  [ 0.02031517  0.03555783  0.03095258 -0.04340066  0.00628262]]]\n"
     ]
    }
   ],
   "source": [
    "# 전처리할 텍스트\n",
    "texts = [\"커피 한잔 어때\", \"오늘 날씨 참 좋네\", \"옷이 어울려요\"]\n",
    "\n",
    "# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# texts_to_sequences로 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 단어 인덱스 확인\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\n단어 인덱스:\\n\", word_index)\n",
    "\n",
    "# 패딩을 통해 시퀀스 길이를 맞춤\n",
    "print(\"\\n시퀀스:\\n\", sequences)\n",
    "padded_sequences = pad_sequences(sequences, 4)\n",
    "print(\"\\n패딩된 시퀀스:\\n\", padded_sequences)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=5, input_length=4))\n",
    "# input_dim에 1을 더하는 것은 인덱스 0을 패딩 값으로 사용하기 위함.\n",
    "# Keras의 Tokenizer는 단어 인덱스를 1부터 시작하기 때문에, 인덱스 0은 패딩 값으로 예약\n",
    "# output_dim은 단어가 임베딩될 벡터의 길이\n",
    "\n",
    "# 임베딩 결과 확인\n",
    "embedding_output = model.predict(padded_sequences)\n",
    "print(\"\\n임베딩 결과:\\n\", embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b29f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
