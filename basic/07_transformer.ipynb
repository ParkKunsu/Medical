{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ead658",
   "metadata": {},
   "source": [
    "# 트랜스포머 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a5b232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:06:20.313526: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 20:06:20.326481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745751980.337955  714596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745751980.340808  714596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745751980.349053  714596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751980.349077  714596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751980.349079  714596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745751980.349079  714596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 20:06:20.352340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# !git clone https://github.com/taehojo/data.git\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv(\"./data/sentiment_data.csv\")\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe[\"sentence\"].tolist()\n",
    "labels = dataframe[\"label\"].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449426cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 함수\n",
    "\n",
    "\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))  # 포지셔널 인코딩 배열 초기화. max_len: 최대 시퀀스 길이, d_model:임베딩 벡터의 차원\n",
    "\n",
    "    # 시퀀스의 각 위치에 대해 포지셔널 인코딩 값 계산\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))  # 짝수 인덱스\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))  # 홀수 인덱스\n",
    "\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326f62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드 어텐션 레이어\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()  # 레이어가 생성될 때 한 번 실행.\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=key_dim\n",
    "        )  # 멀티헤드 어텐션 레이어 생성. num_heads: 어텐션 헤드의 수, key_dim은 각 헤드의 차원 수\n",
    "        self.norm = LayerNormalization()  # 레이어 정규화\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x)  # 멀티헤드 어텐션 적용\n",
    "        attn_output = self.norm(attn_output + x)  # 잔차 연결 적용\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf028d",
   "metadata": {},
   "source": [
    "# 트랜스포머 응용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505ee2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:09:18.568265: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 20:09:18.577987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745752158.588531  716485 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745752158.591374  716485 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745752158.600260  716485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745752158.600286  716485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745752158.600287  716485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745752158.600289  716485 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 20:09:18.603249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kunsu/miniconda3/envs/fast/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1745752162.584599  716485 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745752165.097394  716643 service.cc:152] XLA service 0x7f556401ba00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745752165.097449  716643 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti SUPER, Compute Capability 8.9\n",
      "2025-04-27 20:09:25.172963: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745752165.505686  716643 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-04-27 20:09:27.073254: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19_0', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:27.167888: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1023', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:27.234841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46', 428 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:27.246223: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46', 416 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:27.248186: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:27.342015: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 276 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:28.214155: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:28.365053: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19_0', 3044 bytes spill stores, 4564 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:28.366826: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 4720 bytes spill stores, 4704 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:28.383899: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 400 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "2025-04-27 20:09:28.536827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_19', 5664 bytes spill stores, 5652 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 63/100\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5273 - loss: 0.9174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745752170.883503  716643 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.5219 - loss: 0.8672 - val_accuracy: 0.4975 - val_loss: 0.6478\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8243 - loss: 0.3650 - val_accuracy: 0.9975 - val_loss: 0.0152\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0636 - val_accuracy: 0.9975 - val_loss: 0.0090\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 0.0591 - val_accuracy: 1.0000 - val_loss: 7.8207e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0339 - val_accuracy: 0.9975 - val_loss: 0.0075\n",
      "Epoch 6/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-2s\u001b[0m -16790us/step - accuracy: 0.9965 - loss: 0.0354 - val_accuracy: 1.0000 - val_loss: 0.0057\n",
      "Epoch 7/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 8.9817e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0201 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 9/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0142 - val_accuracy: 0.9900 - val_loss: 0.0257\n",
      "Epoch 10/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0126 - val_accuracy: 1.0000 - val_loss: 2.5907e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:09:33.524109: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_285', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Text: I absolutely love this!\n",
      "Prediction: Negative\n",
      "Text: I can't stand this product\n",
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# !git clone https://github.com/taehojo/data.git\n",
    "\n",
    "# CSV 파일 로드\n",
    "dataframe = pd.read_csv(\"./data/sentiment_data.csv\")\n",
    "\n",
    "# 데이터와 라벨 추출\n",
    "sentences = dataframe[\"sentence\"].tolist()\n",
    "labels = dataframe[\"label\"].tolist()\n",
    "\n",
    "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
    "embedding_dim = 128\n",
    "max_len = 10\n",
    "\n",
    "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 포지셔널 인코딩 함수\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "# 포지셔널 인코딩 생성\n",
    "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
    "\n",
    "\n",
    "# 멀티헤드 어텐션 레이어\n",
    "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, masked=False):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()\n",
    "        self.masked = masked  # 새롭게 추가된 부분\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.masked:\n",
    "            # 마스크드 어텐션을 적용할 경우\n",
    "            batch_size = tf.shape(x)[0]  # 입력 x의 배치 크기\n",
    "            seq_len = tf.shape(x)[1]  # 입력 x의 시퀀스 길이\n",
    "\n",
    "            # 마스크 행렬 생성\n",
    "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # tf.linalg.band_part: 대각선 이전 값들을 1로 채우고 나머지는 0으로\n",
    "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
    "            # 마스크 행렬의 형태를 만들어 주는 부분 -->    # 만약 [1, 1, 4, 4] 라면\n",
    "            # [[1, 0, 0, 0],\n",
    "            #  [1, 1, 0, 0],\n",
    "            #  [1, 1, 1, 0],\n",
    "            #  [1, 1, 1, 1]] 이런 형태로 변환됨\n",
    "\n",
    "            mask = tf.tile(mask, [batch_size, 1, 1, 1])  # 배치 크기만큼 마스크를 반복하여 확장\n",
    "\n",
    "            # 마스크 행렬을 -무한대로 변경\n",
    "            mask = mask * -1e9\n",
    "\n",
    "            # 마스크를 사용한 멀티헤드 어텐션\n",
    "            attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "        else:\n",
    "            # 마스크 없이 멀티헤드 어텐션을 적용할 경우\n",
    "            attn_output = self.mha(query=x, value=x, key=x)\n",
    "\n",
    "        attn_output = self.norm(attn_output + x)  # 잔차 연결 적용 후 레이어 정규화\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# 모델 설정\n",
    "inputs = Input(shape=(max_len,))\n",
    "\n",
    "# 1. 임베딩 레이어: 텍스트 데이터를 임베딩 벡터로 변환\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
    "embedded_sequences = embedding_layer(inputs)\n",
    "\n",
    "# 2. 포지셔널 인코딩 추가\n",
    "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
    "\n",
    "# 3. 멀티헤드 어텐션 레이어 추가\n",
    "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
    "attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
    "\n",
    "# 4. 잔차 연결\n",
    "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
    "\n",
    "# 5. 마스크드 멀티헤드 어텐션 레이어 추가\n",
    "masked_attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim, masked=True)  # masked=True 적용\n",
    "masked_attention_output = masked_attention_layer(attention_output_with_residual)\n",
    "\n",
    "# 6. 잔차 연결\n",
    "masked_attention_output_with_residual = Add()([attention_output_with_residual, masked_attention_output])\n",
    "\n",
    "# 7. GlobalAveragePooling1D 레이어 추가\n",
    "pooled_output = GlobalAveragePooling1D()(masked_attention_output_with_residual)\n",
    "\n",
    "# 8. 피드 포워드 네트워크\n",
    "dense_layer = Dense(128, activation=\"relu\")(pooled_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))\n",
    "\n",
    "# 샘플 데이터 예측\n",
    "sample_texts = [\"I absolutely love this!\", \"I can't stand this product\"]\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding=\"post\")\n",
    "predictions = model.predict(sample_data)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
